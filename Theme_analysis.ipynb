{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juli1001/Theme_Analysis/blob/main/Theme_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "F3BVy-Ev0o8n",
        "outputId": "a15b0407-960f-46d4-e477-cfea7401b9d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.3-py3-none-any.whl (327 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/327.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/327.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m317.4/327.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key='###')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_ndba0f1Edm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UVMpINloR_1"
      },
      "source": [
        "# **Data Frame from data base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vkq7mmGp1qkU",
        "outputId": "0b400576-1c48-48a6-eb7e-bae359afca20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-8.4.0-cp310-cp310-manylinux_2_17_x86_64.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-8.4.0\n",
            "Collecting plotly==5.3.1\n",
            "  Downloading plotly-5.3.1-py2.py3-none-any.whl (23.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly==5.3.1) (8.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from plotly==5.3.1) (1.16.0)\n",
            "Installing collected packages: plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.15.0\n",
            "    Uninstalling plotly-5.15.0:\n",
            "      Successfully uninstalled plotly-5.15.0\n",
            "Successfully installed plotly-5.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mysql-connector-python\n",
        "!pip install plotly==5.3.1\n",
        "from sqlalchemy import create_engine, text\n",
        "connection_string = \"mysql+mysqlconnector://####\"\n",
        "engine = create_engine(connection_string, echo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uC_sNrLA1OpT",
        "outputId": "aaa23a37-a649-4684-9374-002c9a4de919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:15,961 INFO sqlalchemy.engine.Engine SELECT DATABASE()\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT DATABASE()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:15,963 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[raw sql] {}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:16,679 INFO sqlalchemy.engine.Engine SELECT @@sql_mode\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT @@sql_mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:16,681 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[raw sql] {}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:17,040 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT @@lower_case_table_names\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:17,042 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[raw sql] {}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:17,578 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:BEGIN (implicit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:17,583 INFO sqlalchemy.engine.Engine SELECT l.assistant_name, l.business_name,c.status, c.from ,c.start_datetime, c.end_datetime, c.listing_id AS conversation_listing_id, c.id_channel, TIMESTAMPDIFF(MINUTE, c.start_datetime, c.end_datetime) AS conversation_duration, TIME_TO_SEC(TIMEDIFF(c.end_datetime, c.start_datetime)) / 60 AS conversation_duration_minutes, CASE c.id_channel WHEN 1 THEN 'voice' WHEN 2 THEN 'text' WHEN 3 THEN 'whatsapp' ELSE 'otros' END AS channel_name, c.messages FROM AI_HELLOU_DB.listing AS l JOIN AI_HELLOU_DB.conversation AS c ON l.id = c.listing_id;\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:SELECT l.assistant_name, l.business_name,c.status, c.from ,c.start_datetime, c.end_datetime, c.listing_id AS conversation_listing_id, c.id_channel, TIMESTAMPDIFF(MINUTE, c.start_datetime, c.end_datetime) AS conversation_duration, TIME_TO_SEC(TIMEDIFF(c.end_datetime, c.start_datetime)) / 60 AS conversation_duration_minutes, CASE c.id_channel WHEN 1 THEN 'voice' WHEN 2 THEN 'text' WHEN 3 THEN 'whatsapp' ELSE 'otros' END AS channel_name, c.messages FROM AI_HELLOU_DB.listing AS l JOIN AI_HELLOU_DB.conversation AS c ON l.id = c.listing_id;\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-06-24 19:55:17,592 INFO sqlalchemy.engine.Engine [generated in 0.19145s] {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sqlalchemy.engine.Engine:[generated in 0.19145s] {}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     assistant_name                           structured_conversations\n",
            "0             Julia  User: alumina service  made when I started are...\n",
            "1             Julia       User:  \\nUser: Columbia.\\nUser: Will change.\n",
            "2             Julia                            User: same with that is\n",
            "3             Julia  User: When I saw this.  even when I start a nu...\n",
            "4             Julia  User:  Hello Mrs.\\nAsistente: Koury, I am inte...\n",
            "...             ...                                                ...\n",
            "2274   Green Tomato  User: Tienes tomates earloom?\\nAsistente: Sí, ...\n",
            "2275   Green Tomato  User: Test 2\\nAsistente: Hello! How can I assi...\n",
            "2276   Green Tomato  User: hola… que vegetales tienes?\\nAsistente: ...\n",
            "2277   Green Tomato  User: Yes, I would like to know if you deliver...\n",
            "2278   Green Tomato  User: hello\\nAsistente: Hello! How can I assis...\n",
            "\n",
            "[2279 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "result = engine.connect().execute(text(\"SELECT l.assistant_name, l.business_name,c.status, c.from ,c.start_datetime, c.end_datetime, c.listing_id AS conversation_listing_id, c.id_channel, TIMESTAMPDIFF(MINUTE, c.start_datetime, c.end_datetime) AS conversation_duration, TIME_TO_SEC(TIMEDIFF(c.end_datetime, c.start_datetime)) / 60 AS conversation_duration_minutes, CASE c.id_channel WHEN 1 THEN 'voice' WHEN 2 THEN 'text' WHEN 3 THEN 'whatsapp' ELSE 'otros' END AS channel_name, c.messages FROM AI_HELLOU_DB.listing AS l JOIN AI_HELLOU_DB.conversation AS c ON l.id = c.listing_id;\"))\n",
        "data = [tuple(row) for row in result]\n",
        "columns = result.keys()\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Parsea la cadena JSON en la columna \"messages\"\n",
        "df['messages'] = df['messages'].apply(lambda x: json.loads(x))\n",
        "\n",
        "# Crear una nueva columna con las conversaciones estructuradas\n",
        "structured_conversations = []\n",
        "for message_list in df['messages']:\n",
        "    conversation = \"\"\n",
        "    for msg in message_list:\n",
        "        role = msg['role']\n",
        "        content = msg['content']\n",
        "        if role == 'user':\n",
        "            conversation += f\"User: {content}\\n\"\n",
        "        elif role == 'assistant':\n",
        "            conversation += f\"Asistente: {content}\\n\"\n",
        "    structured_conversations.append(conversation.strip())\n",
        "\n",
        "\n",
        "df['structured_conversations'] = structured_conversations\n",
        "\n",
        "\n",
        "print(df[['assistant_name', 'structured_conversations']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9LtOdzJ1YZv"
      },
      "outputs": [],
      "source": [
        "def extraer_mensajes_usuario(texto_conversacion):\n",
        "    lineas = texto_conversacion.split('\\n')\n",
        "    mensajes_usuario = [linea for linea in lineas if linea.startswith('User:')]\n",
        "    return ' '.join(mensajes_usuario)\n",
        "\n",
        "df['mensajes_usuario'] = df['structured_conversations'].apply(extraer_mensajes_usuario)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p94Lndk53Q0b",
        "outputId": "dd1cd2cc-07a1-4274-84cc-5fc5cac88939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Tienes tomates earloom?\n",
            "Asistente: Sí, tenemos tomates Heirloom en nuestra selección de productos frescos. Puedes encontrarlos en nuestras bolsas de frutas y verduras mixtas. ¡Suscríbete para recibirlos en tu próxima entrega!\n",
            "User: Cuánto valen?\n",
            "Asistente: El precio de nuestras bolsas de frutas y verduras mixtas varía según el tamaño que elijas. Nuestros precios van desde $XX para la bolsa Cherry hasta $XX para la bolsa Beefsteak. Puedes encontrar más detalles sobre los precios en nuestro sitio web o contactarnos directamente para obtener información específica sobre los precios actuales.\n"
          ]
        }
      ],
      "source": [
        "print(df.iloc[2274]['structured_conversations'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cLZG6-qmuBe3",
        "outputId": "83246345-a5d2-4165-bf5f-58d29559d987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2260    User: when did you purchase this car?\\nAsisten...\n",
            "2261    User: what is special about this car?\\nAsisten...\n",
            "2262    User: Yes, who is the original owner of the ca...\n",
            "2263                                                     \n",
            "2264    User: que es un desoto?\\nAsistente: DeSoto es ...\n",
            "2265    User: Hola! Que es un DeSoto\\nAsistente: Hola!...\n",
            "2266                                                     \n",
            "2267    User: Hi there\\nAsistente: Hello! How can I as...\n",
            "2268                                                     \n",
            "2269    User:  Shayla where are you located?\\nAsistent...\n",
            "2270    User: Hi there, can you tell me more about gre...\n",
            "2271    User: WhatsApp code 584-947\\nAsistente: I apol...\n",
            "2272    User: I live in Miami Beach, when do you deliv...\n",
            "2273    User: “I got zucchini, bed bell peppers and to...\n",
            "2274    User: Tienes tomates earloom?\\nAsistente: Sí, ...\n",
            "2275    User: Test 2\\nAsistente: Hello! How can I assi...\n",
            "Name: structured_conversations, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df.iloc[2260:2276]['structured_conversations'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfzilaWioiuz"
      },
      "source": [
        "# **The theme generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSjTdLC0_fnf",
        "outputId": "7a3c6948-6831-4423-a143-75a0295a4a97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                               structured_conversations  \\\n",
            "2260  User: when did you purchase this car?\\nAsisten...   \n",
            "2261  User: what is special about this car?\\nAsisten...   \n",
            "2262  User: Yes, who is the original owner of the ca...   \n",
            "2263                                                      \n",
            "2264  User: que es un desoto?\\nAsistente: DeSoto es ...   \n",
            "2265  User: Hola! Que es un DeSoto\\nAsistente: Hola!...   \n",
            "2266                                                      \n",
            "2267  User: Hi there\\nAsistente: Hello! How can I as...   \n",
            "2268                                                      \n",
            "2269  User:  Shayla where are you located?\\nAsistent...   \n",
            "2270  User: Hi there, can you tell me more about gre...   \n",
            "2271  User: WhatsApp code 584-947\\nAsistente: I apol...   \n",
            "2272  User: I live in Miami Beach, when do you deliv...   \n",
            "2273  User: “I got zucchini, bed bell peppers and to...   \n",
            "2274  User: Tienes tomates earloom?\\nAsistente: Sí, ...   \n",
            "2275  User: Test 2\\nAsistente: Hello! How can I assi...   \n",
            "2276  User: hola… que vegetales tienes?\\nAsistente: ...   \n",
            "2277  User: Yes, I would like to know if you deliver...   \n",
            "\n",
            "                       Theme  \n",
            "2260          family history  \n",
            "2261          Family history  \n",
            "2262       Ownership history  \n",
            "2263                     N/A  \n",
            "2264             Car History  \n",
            "2265               Car Brand  \n",
            "2266                     N/A  \n",
            "2267  Fresh Produce Delivery  \n",
            "2268                     N/A  \n",
            "2269    business information  \n",
            "2270   \\n    organic produce  \n",
            "2271        WhatsApp Support  \n",
            "2272    Subscription service  \n",
            "2273      Recipe Suggestions  \n",
            "2274     pricing information  \n",
            "2275                     N/A  \n",
            "2276       Delivery Schedule  \n",
            "2277    delivery information  \n"
          ]
        }
      ],
      "source": [
        "def analizar_Theme(conversacion):\n",
        "    prompt = f\"\"\"\n",
        "    Eres un modelo de IA entrenado para analizar el tema de las conversaciones.\n",
        "    Tu tarea es identificar el tema principal en una conversación estructurada entre un usuario y un asistente.\n",
        "\n",
        "    Ejemplo:\n",
        "    User: Tienes tomates Heirloom?\n",
        "    Asistente: Sí, tenemos tomates Heirloom en nuestra selección de productos frescos. Puedes encontrarlos en nuestras bolsas de frutas y verduras mixtas. ¡Suscríbete para recibirlos en tu próxima entrega!\n",
        "    User: Cuánto valen?\n",
        "    Asistente: El precio de nuestras bolsas de frutas y verduras mixtas varía según el tamaño que elijas. Nuestros precios van desde $XX para la bolsa Cherry hasta $XX para la bolsa Beefsteak. Puedes encontrar más detalles sobre los precios en nuestro sitio web.\n",
        "    pricing information\n",
        "\n",
        "    Analiza la siguiente conversación y determina el tema principal de la misma. El tema debe estar en inglés y ser de 2-3 palabras. Si la conversación está vacía, por favor déjalo en blanco o coloca N/A:\n",
        "\n",
        "    {conversacion['structured_conversations']}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un modelo de IA entrenado para analizar el tema de las conversaciones.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message\n",
        "\n",
        "# Supongamos que tu DataFrame ya está definido como df\n",
        "df_2 = df.iloc[2260:2278].copy()\n",
        "\n",
        "# Añadir columna 'Theme' al DataFrame\n",
        "df_2['Theme'] = df_2.apply(lambda row: analizar_Theme({\n",
        "    \"structured_conversations\": row['structured_conversations']\n",
        "}), axis=1)\n",
        "\n",
        "print(df_2[['structured_conversations', 'Theme']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuadtYxPo23_"
      },
      "source": [
        "# **Here I did a text (didn't work to well)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAhTctkQVm4C",
        "outputId": "eb5e8e96-9ea4-4a02-e06c-134f32bf13fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¿Qué temas quieres generar? (separados por comas) Deliveries, Business, Product, Recipe\n",
            "                               structured_conversations               Theme\n",
            "2260  User: when did you purchase this car?\\nAsisten...         Car History\n",
            "2261  User: what is special about this car?\\nAsisten...  Theme: Car History\n",
            "2262  User: Yes, who is the original owner of the ca...           Ownership\n",
            "2263                                                                    N/A\n",
            "2264  User: que es un desoto?\\nAsistente: DeSoto es ...          Automobile\n",
            "2265  User: Hola! Que es un DeSoto\\nAsistente: Hola!...         Automobiles\n",
            "2266                                                                    N/A\n",
            "2267  User: Hi there\\nAsistente: Hello! How can I as...   Delivery Services\n",
            "2268                                                                    N/A\n",
            "2269  User:  Shayla where are you located?\\nAsistent...   Delivery Services\n",
            "2270  User: Hi there, can you tell me more about gre...   Delivery Schedule\n",
            "2271  User: WhatsApp code 584-947\\nAsistente: I apol...   Delivery Services\n",
            "2272  User: I live in Miami Beach, when do you deliv...   Delivery Schedule\n",
            "2273  User: “I got zucchini, bed bell peppers and to...             Recipes\n",
            "2274  User: Tienes tomates earloom?\\nAsistente: Sí, ...     Produce Pricing\n",
            "2275  User: Test 2\\nAsistente: Hello! How can I assi...                 N/A\n",
            "2276  User: hola… que vegetales tienes?\\nAsistente: ...   Delivery Schedule\n",
            "2277  User: Yes, I would like to know if you deliver...   Delivery Schedule\n"
          ]
        }
      ],
      "source": [
        "openai.api_key='####'\n",
        "\n",
        "def analizar_Theme(conversacion, tema):\n",
        "    prompt = f\"\"\"\n",
        "    Eres un modelo de IA especializado en analizar conversaciones.\n",
        "    Tu tarea es identificar el tema principal de una conversación estructurada entre un usuario y un asistente, poniendo especial atención en lo que solicita el usuario.\n",
        "\n",
        "    Por favor, analiza la siguiente conversación y determina el tema principal en relación a \"{tema}\". El tema debe estar en inglés y ser de 1-2 palabras. Si la conversación está vacía, por favor déjalo en blanco o coloca N/A.\n",
        "\n",
        "    Ejemplo 1:\n",
        "    Conversación:\n",
        "    User: Do you have heirloom tomatoes?\n",
        "    Assistant: Yes, we have heirloom tomatoes in our fresh produce selection. You can find them in our mixed fruit and vegetable bags. Subscribe to receive them in your next delivery!\n",
        "    User: How much do they cost?\n",
        "    Assistant: The price of our mixed fruit and vegetable bags varies depending on the size you choose. Our prices range from $XX for the Cherry bag to $XX for the Beefsteak bag. You can find more details about the prices on our website.\n",
        "    Produce Pricing\n",
        "\n",
        "    Ejemplo 2:\n",
        "    Conversación:\n",
        "    User: What are your delivery hours?\n",
        "    Assistant: We deliver from 8 AM to 8 PM, Monday through Saturday.\n",
        "    User: Can you deliver on Sunday?\n",
        "    Assistant: Unfortunately, we do not deliver on Sundays.\n",
        "    Delivery Schedule\n",
        "\n",
        "    Ahora analiza la siguiente conversación:\n",
        "\n",
        "    {conversacion['structured_conversations']}\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un modelo de IA entrenado para analizar el tema de las conversaciones.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message.content.strip()\n",
        "    return response_message\n",
        "\n",
        "# Preguntar al usuario qué temas quiere generar\n",
        "temas_usuario = input(\"¿Qué temas quieres generar? (separados por comas) \")\n",
        "\n",
        "# Seleccionar un solo tema de la lista ingresada por el usuario\n",
        "tema_seleccionado = temas_usuario.split(',')[0].strip()\n",
        "\n",
        "# Supongamos que tu DataFrame ya está definido como df\n",
        "df_2 = df.iloc[2260:2278].copy()\n",
        "\n",
        "# Añadir columna 'Theme' al DataFrame\n",
        "df_2['Theme'] = df_2.apply(lambda row: analizar_Theme({\n",
        "    \"structured_conversations\": row['structured_conversations']\n",
        "}, tema_seleccionado), axis=1)\n",
        "\n",
        "print(df_2[['structured_conversations', 'Theme']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCN2-j6SpYaC"
      },
      "source": [
        "# **Input of conversation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64qfSXa2HXGY"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Coversacion:\n",
        "User: hello, I had and issue with my delivery?\n",
        "Assitant:I'm sorry to hear that you've had an issue with your delivery. Can you please provide me with your order number and a brief description of the problem? I'll do my best to assist you.\n",
        "User: the order number is #6572, what can I do?\n",
        "Assistant:Thank you for providing your order number #6572. Let's resolve this issue for you.\n",
        "Verify Order Details: I'll check the status and details of your order.\n",
        "Describe the Issue: Please let me know what specifically went wrong with your delivery (e.g., missing items, wrong items, damaged produce).\n",
        "Resolution Options: Depending on the issue, we can offer various resolutions such as a replacement, a refund, or a discount on your next purchase.\n",
        "Could you please provide more details about the problem with your delivery?\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPw68v5ioELy",
        "outputId": "041b43d3-58d4-4df1-a0b8-ce744693733a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Por favor, ingresa una conversación estructurada:\n",
            "User: hello, I had and issue with my delivery? Assitant:I'm sorry to hear that you've had an issue with your delivery. Can you please provide me with your order number and a brief description of the problem? I'll do my best to assist you. User: the order number is #6572, what can I do? Assistant:Thank you for providing your order number #6572. Let's resolve this issue for you. Verify Order Details: I'll check the status and details of your order. Describe the Issue: Please let me know what specifically went wrong with your delivery (e.g., missing items, wrong items, damaged produce). Resolution Options: Depending on the issue, we can offer various resolutions such as a replacement, a refund, or a discount on your next purchase. Could you please provide more details about the problem with your delivery?\n",
            "                             structured_conversations  \\\n",
            "0   User: when did you purchase this car?\\nAsisten...   \n",
            "1   User: what is special about this car?\\nAsisten...   \n",
            "2   User: Yes, who is the original owner of the ca...   \n",
            "3                                                       \n",
            "4   User: que es un desoto?\\nAsistente: DeSoto es ...   \n",
            "5   User: Hola! Que es un DeSoto\\nAsistente: Hola!...   \n",
            "6                                                       \n",
            "7   User: Hi there\\nAsistente: Hello! How can I as...   \n",
            "8                                                       \n",
            "9   User:  Shayla where are you located?\\nAsistent...   \n",
            "10  User: Hi there, can you tell me more about gre...   \n",
            "11  User: WhatsApp code 584-947\\nAsistente: I apol...   \n",
            "12  User: I live in Miami Beach, when do you deliv...   \n",
            "13  User: “I got zucchini, bed bell peppers and to...   \n",
            "14  User: Tienes tomates earloom?\\nAsistente: Sí, ...   \n",
            "15  User: Test 2\\nAsistente: Hello! How can I assi...   \n",
            "16  User: hola… que vegetales tienes?\\nAsistente: ...   \n",
            "17  User: Yes, I would like to know if you deliver...   \n",
            "18  User: hello, I had and issue with my delivery?...   \n",
            "\n",
            "                        Theme  \n",
            "0              Family History  \n",
            "1              Theme: Vintage  \n",
            "2               Car Ownership  \n",
            "3                         N/A  \n",
            "4                 Car History  \n",
            "5            Automobile Brand  \n",
            "6                         N/A  \n",
            "7        Subscription Service  \n",
            "8                         N/A  \n",
            "9       Green Tomato Business  \n",
            "10  Green Tomato Subscription  \n",
            "11           WhatsApp Support  \n",
            "12          Delivery Schedule  \n",
            "13               Recipe Ideas  \n",
            "14            Product Pricing  \n",
            "15                        N/A  \n",
            "16          Delivery Schedule  \n",
            "17          Delivery Schedule  \n",
            "18             Delivery Issue  \n"
          ]
        }
      ],
      "source": [
        "openai.api_key='####'\n",
        "\n",
        "def analizar_Theme(conversacion):\n",
        "    prompt = f\"\"\"\n",
        "    Eres un modelo de IA especializado en analizar conversaciones.\n",
        "    Tu tarea es identificar el tema principal de una conversación estructurada entre un usuario y un asistente, poniendo especial atención en lo que solicita el usuario.\n",
        "\n",
        "    Por favor, analiza la siguiente conversación y determina el tema principal. El tema debe estar en inglés y ser de 1-3 palabras. Si la conversación está vacía, por favor déjalo en blanco o coloca N/A. No puede incluir la palabra Topic al principio.\n",
        "\n",
        "    Ejemplo 1:\n",
        "    Conversación:\n",
        "    User: Do you have heirloom tomatoes?\n",
        "    Assistant: Yes, we have heirloom tomatoes in our fresh produce selection. You can find them in our mixed fruit and vegetable bags. Subscribe to receive them in your next delivery!\n",
        "    User: How much do they cost?\n",
        "    Assistant: The price of our mixed fruit and vegetable bags varies depending on the size you choose. Our prices range from $XX for the Cherry bag to $XX for the Beefsteak bag. You can find more details about the prices on our website.\n",
        "    Product Pricing\n",
        "\n",
        "    Ejemplo 2:\n",
        "    Conversación:\n",
        "    User: What are your delivery hours?\n",
        "    Assistant: We deliver from 8 AM to 8 PM, Monday through Saturday.\n",
        "    User: Can you deliver on Sunday?\n",
        "    Assistant: Unfortunately, we do not deliver on Sundays.\n",
        "    Delivery Schedule\n",
        "\n",
        "    Ahora analiza la siguiente conversación:\n",
        "\n",
        "    {conversacion['structured_conversations']}\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un modelo de IA especializado en analizar conversaciones.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message.content.strip()\n",
        "    return response_message\n",
        "\n",
        "# Input\n",
        "input_conversation = input(\"Por favor, ingresa una conversación estructurada:\\n\")\n",
        "\n",
        "# Crear un DataFrame con la conversación ingresada\n",
        "data = {\n",
        "    'structured_conversations': [input_conversation]\n",
        "}\n",
        "df_input = pd.DataFrame({'structured_conversations': [input_conversation]})\n",
        "df_existing = df.iloc[2260:2278].copy()\n",
        "df_combined = pd.concat([df_existing, df_input], ignore_index=True)\n",
        "\n",
        "# Añadir columna 'Theme' al DataFrame combinado\n",
        "df_combined['Theme'] = df_combined.apply(lambda row: analizar_Theme({\n",
        "    \"structured_conversations\": row['structured_conversations']\n",
        "}), axis=1)\n",
        "\n",
        "print(df_combined[['structured_conversations', 'Theme']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYVR0X2sYaRn"
      },
      "source": [
        "# **API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Immetv7HoZF"
      },
      "source": [
        "No esta conectada al df, pero desde postman se le puede hacer un input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SzIUWqqHwbf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Input:\n",
        "{\n",
        "  \"structured_conversations\": \"User: What are your delivery hours? Assistant: We deliver from 8 AM to 8 PM, Monday through Saturday. User: Can you deliver on Sunday? Assistant: Unfortunately, we do not deliver on Sundays.\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "Output:\n",
        "{\n",
        "    \"theme\": \"Delivery Schedule\"\n",
        "}\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfYSGo0aGXZ0",
        "outputId": "9bb3921b-92dd-4c37-a2b0-02db3e10a44a",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.3)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.4)\n",
            "Collecting python-multipart>=0.0.7 (from fastapi)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.2.1 (from fastapi)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n",
            "Installing collected packages: websockets, uvloop, uvicorn, ujson, python-multipart, python-dotenv, pyngrok, orjson, httptools, dnspython, watchfiles, starlette, email_validator, fastapi-cli, fastapi\n",
            "Successfully installed dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 httptools-0.6.1 orjson-3.10.5 pyngrok-7.1.6 python-dotenv-1.0.1 python-multipart-0.0.9 starlette-0.37.2 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn openai nest_asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vDCnQ41Wq_81",
        "outputId": "584de35a-1cc8-488b-f4f3-146ac4ddfdbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://9b2b-34-80-35-202.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [194]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     73.205.34.153:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     73.205.34.153:0 - \"GET /analyze_theme HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     73.205.34.153:0 - \"GET /analyze_theme/ HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     73.205.34.153:0 - \"GET /analyze_theme HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     73.205.34.153:0 - \"GET /analyze_theme/ HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     73.205.34.153:0 - \"GET /analyze_theme HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     73.205.34.153:0 - \"GET /analyze_theme/ HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     73.205.34.153:0 - \"POST /analyze_theme HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     73.205.34.153:0 - \"POST /analyze_theme/ HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import openai\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "openai.api_key = '###'\n",
        "ngrok.set_auth_token('###')\n",
        "app = FastAPI()\n",
        "\n",
        "class ConversationRequest(BaseModel):\n",
        "    structured_conversations: str\n",
        "\n",
        "def analizar_Theme(conversacion: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    Eres un modelo de IA especializado en analizar conversaciones.\n",
        "    Tu tarea es identificar el tema principal de una conversación estructurada entre un usuario y un asistente, poniendo especial atención en lo que solicita el usuario.\n",
        "\n",
        "    Por favor, analiza la siguiente conversación y determina el tema principal. El tema debe estar en inglés y ser de 1-3 palabras. Si la conversación está vacía, por favor déjalo en blanco o coloca N/A. No puede incluir Topic al principio.\n",
        "\n",
        "    Ejemplo 1:\n",
        "    Conversación:\n",
        "    User: Do you have heirloom tomatoes?\n",
        "    Assistant: Yes, we have heirloom tomatoes in our fresh produce selection. You can find them in our mixed fruit and vegetable bags. Subscribe to receive them in your next delivery!\n",
        "    User: How much do they cost?\n",
        "    Assistant: The price of our mixed fruit and vegetable bags varies depending on the size you choose. Our prices range from $XX for the Cherry bag to $XX for the Beefsteak bag. You can find more details about the prices on our website.\n",
        "    Product Pricing\n",
        "\n",
        "    Ejemplo 2:\n",
        "    Conversación:\n",
        "    User: What are your delivery hours?\n",
        "    Assistant: We deliver from 8 AM to 8 PM, Monday through Saturday.\n",
        "    User: Can you deliver on Sunday?\n",
        "    Assistant: Unfortunately, we do not deliver on Sundays.\n",
        "    Delivery Schedule\n",
        "\n",
        "    Ahora analiza la siguiente conversación:\n",
        "\n",
        "    {conversacion}\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un modelo de IA entrenado para analizar el tema de las conversaciones.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message\n",
        "\n",
        "@app.post(\"/analyze_theme/\")\n",
        "async def analyze_theme(request: ConversationRequest):\n",
        "    try:\n",
        "        theme = analizar_Theme(request.structured_conversations)\n",
        "        return {\"theme\": theme}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run():\n",
        "\n",
        "    public_url = ngrok.connect(addr=\"8000\")\n",
        "    print(f\"Public URL: {public_url}\")\n",
        "    uvicorn.run(app, host='0.0.0.0', port=8000)\n",
        "\n",
        "# Ejecutar el servidor y el túnel ngrok\n",
        "run()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}